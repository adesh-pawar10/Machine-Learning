{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cb8bb7c-04d8-4f0d-9e30-38303ee89606",
   "metadata": {},
   "source": [
    "<h1 style = \"color : dodgerblue\"> Naive Bayes Classification </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6896955f-2c13-4159-882a-ba9ac794f4a0",
   "metadata": {},
   "source": [
    "<h2 style = \"color : DeepSkyBlue\"> An Overview of Naive Bayes Classification </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1ad6c9-de9b-4905-9346-52847cd588bc",
   "metadata": {},
   "source": [
    "* Naive Bayes classification is a probabilistic machine learning algorithm that's particularly useful for classification tasks.\n",
    "\n",
    "* It's based on Bayes' Theorem and is called \"naive\" because it makes a simplifying assumption: it assumes that the features in a dataset are independent of each other.\n",
    "\n",
    "* This assumption is often unrealistic in real-world situations, but it simplifies the computation and can still produce good results in many cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91718733-a3e8-4305-bd05-7e71d4c2459d",
   "metadata": {},
   "source": [
    "<h2 style = \"color : DeepSkyBlue\"> What is Naive Bayes Classifiers? </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bf89f6-1897-40ed-a4da-215728469854",
   "metadata": {},
   "source": [
    "* Naive Bayes classifiers are a collection of classification algorithms based on Bayes' Theorem. It is not a single algorithm but a family of algorithms where all of them share a common principle, i.e. every pair of features being classified is independent of each other. To start with, let us consider a dataset.\n",
    "\n",
    "* One of the most simple and effective classification algorithms, the Na√Øve Bayes classifier aids in the rapid development of machine learning models with rapid prediction capabilities.\n",
    "\n",
    "* Na√Øve Bayes algorithm is used for classification problems. It is highly used in text classification. In text classification tasks, data contains high dimension (as each word represent one feature in the data). It is used in spam filtering, sentiment detection, rating classification etc. The advantage of using na√Øve Bayes is its speed. It is fast and making prediction is easy with high dimension of data.\n",
    "\n",
    "* This model predicts the probability of an instance belongs to a class with a given set of feature value. It is a probabilistic classifier. It is because it assumes that one feature in the model is independent of existence of another feature. In other words, each feature contributes to the predictions with no relation between each other. In real world, this condition satisfies rarely. It uses Bayes theorem in the algorithm for training and prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d497544d-7c0d-4fab-9c8c-ae155256153e",
   "metadata": {},
   "source": [
    "<h2 style = \"color : DeepSkyBlue\"> Why it is Called Naive Bayes? </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba12615-5d4c-4535-820f-58495b971524",
   "metadata": {},
   "source": [
    "It's called \"Naive Bayes\" because of its simplifying assumption about the data it handles.\n",
    "\n",
    "* <b style = \"color : orangered\">Naive:</b> The term \"naive\" refers to the assumption that the features used in the model are independent of each other. In reality, this is often not true‚Äîfeatures can be correlated. However, this assumption simplifies the computation and, despite being naive, the classifier often performs surprisingly well in practice.\n",
    "\n",
    "* <b style = \"color : orangered\"> Bayes: </b> The \"Bayes\" part of the name comes from Bayes' Theorem, which underpins the algorithm. Bayes' Theorem allows us to update our prior beliefs about the probability of a hypothesis based on new evidence. Naive Bayes uses this theorem to calculate the probability of each class given the data.\n",
    "\n",
    "So, putting it all together, Naive Bayes is a probabilistic model that makes a \"naive\" assumption of independence among features and leverages Bayes' Theorem to perform classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd89acd0-4541-4fac-a6e7-76693573fab8",
   "metadata": {},
   "source": [
    "<h2 style = \"color : DeepSkyBlue\"> Bayes' Theorem </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61256196-7b81-4270-a7f8-ad70db489c4e",
   "metadata": {},
   "source": [
    "Bayes' Theorem is the foundation of Naive Bayes classification. It describes the probability of an event, based on prior knowledge of conditions that might be related to the event. The theorem is stated as:\n",
    "\n",
    "ùëÉ(ùê¥‚à£ùêµ) = ùëÉ(ùêµ‚à£ùê¥) * ùëÉ(ùê¥) / ùëÉ(ùêµ)\n",
    "\n",
    "Where:\n",
    "\n",
    "* ùëÉ(ùê¥‚à£ùêµ) is the posterior probability of class ùê¥ given the evidence ùêµ.\n",
    "\n",
    "* P(B|A) is the likelihood of the evidence given class ùê¥.\n",
    "\n",
    "* P(A) is the prior probability of class ùê¥.\n",
    "\n",
    "* P(B) is the prior probability of the evidence ùêµ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3303a0-d2ee-47e9-9dd8-62d78fb834b7",
   "metadata": {},
   "source": [
    "<h2 style = \"color : DeepSkyBlue\"> Naive Bayes Classifier </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858b2aaa-e886-4732-88b4-0ad44292f171",
   "metadata": {},
   "source": [
    "Naive Bayes classifiers apply Bayes' Theorem with the \"naive\" assumption of conditional independence between every pair of features given the value of the class variable.\n",
    "\n",
    "<b style = \"color : coral\">Steps to Implement Naive Bayes Classification</b>\n",
    "\n",
    "<b style = \"color : orangered\">1. Calculate the Prior Probability:</b> This is the initial probability of each class without any evidence. It can be estimated from the training data as the proportion of each class.\n",
    "\n",
    "<b style = \"color : orangered\">2. Calculate the Likelihood:</b> For each feature in the data, calculate the likelihood of the feature value given each class. This involves estimating the probability distribution of the features.\n",
    "\n",
    "<b style = \"color : orangered\">3. Calculate the Posterior Probability:</b> Use Bayes' Theorem to calculate the posterior probability of each class given the feature values of a new instance.\n",
    "\n",
    "<b style = \"color : orangered\">4. Predict the Class:</b> The class with the highest posterior probability is chosen as the predicted class for the new instance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "72b31a8e-9872-4854-b83c-34a47ef82f15",
   "metadata": {},
   "source": [
    "<h2 style = \"color : DeepSkyBlue\"> Types of Naive Bayes Classifiers </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7c66da-1c0f-422b-92c1-ab2c9796be6d",
   "metadata": {},
   "source": [
    "<b style = \"color : orangered\">1.Gaussian Naive Bayes:</b> Assumes that the features follow a normal (Gaussian) distribution.\n",
    "\n",
    "<b style = \"color : orangered\">2. Multinomial Naive Bayes:</b> Used for discrete features like word counts in text classification problems.\n",
    "\n",
    "<b style = \"color : orangered\">3. Bernoulli Naive Bayes:</b> Used for binary/boolean features."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f5d61b60-b808-41b6-9df8-16c3764a45bc",
   "metadata": {},
   "source": [
    "<h2 style = \"color : DeepSkyBlue\"> Example </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d7ec86-0a77-4a45-a3d9-9a3ffc671156",
   "metadata": {},
   "source": [
    "<b style = \"color : orangered\"> Imagine we want to classify whether an email is spam or not spam based on the presence of certain keywords. Let's say our features are \"offer,\" \"win,\" and \"free\": </b>\n",
    "\n",
    "1. Calculate the prior probability of spam and not spam from the training data.\n",
    "\n",
    "2. For each keyword, calculate the likelihood of the keyword appearing in spam and not spam emails.\n",
    "\n",
    "3. Given a new email, use Bayes' Theorem to compute the posterior probability of the email being spam or not spam based on the presence of these keywords.\n",
    "\n",
    "4. Classify the email as spam or not spam based on the higher posterior probability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9f9817-e30c-410c-bc01-d1225b13a91d",
   "metadata": {},
   "source": [
    "<h2 style = \"color : DeepSkyBlue\"> Advantages of Naive Bayes </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb49e678-ff0a-4cc5-a361-a5aa8359a5ed",
   "metadata": {},
   "source": [
    "<b style = \"color : orangered\">1. Simple and Fast:</b> Easy to implement and computationally efficient.\n",
    "\n",
    "<b style = \"color : orangered\">2. Scalable:</b> Works well with large datasets.\n",
    "\n",
    "<b style = \"color : orangered\">3. Handles Missing Data:</b> Can handle missing data well by ignoring the missing values during the classification."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "34dd3482-f6e5-46ee-81f6-5282734e76c1",
   "metadata": {},
   "source": [
    "<h2 style = \"color : DeepSkyBlue\"> Limitations of Naive Bayes </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82345b32-c642-4d1c-8dae-7a0d38426a21",
   "metadata": {},
   "source": [
    "<b style = \"color : orangered\"> 1. Independence Assumption: </b> The assumption that features are independent is rarely true in real-world data.\n",
    "\n",
    "<b style = \"color : orangered\"> 2. Zero Probability: </b> If a feature value was not seen in the training data for a given class, it can lead to zero probability, but this can be handled with techniques like Laplace smoothing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fee18bd-b228-43c5-a1ba-3c1e3b5ad871",
   "metadata": {},
   "source": [
    "<h2 style = \"color : DeepSkyBlue\"> Applications </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ac9ca1-1251-4393-822a-04453b490f07",
   "metadata": {},
   "source": [
    "<b style = \"color : orangered\">1. Spam Email Filtering:</b> Classifies emails as spam or non-spam based on features.\n",
    "\n",
    "<b style = \"color : orangered\">2. Text Classification:</b> Used in sentiment analysis, document categorization, and topic classification.\n",
    "\n",
    "<b style = \"color : orangered\">3. Medical Diagnosis:</b> Helps in predicting the likelihood of a disease based on symptoms.\n",
    "\n",
    "<b style = \"color : orangered\">4. Credit Scoring:</b> Evaluates creditworthiness of individuals for loan approval.\n",
    "\n",
    "<b style = \"color : orangered\">5. Weather Prediction:</b> Classifies weather conditions based on various factors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b539865-88ed-4f0c-8a47-954c0f58e208",
   "metadata": {},
   "source": [
    "<h2 style = \"color : DeepSkyBlue\"> Example </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bd53d71-2a61-44f8-8f94-8e1d4bcdc1a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           1.00         1\n",
      "   macro avg       1.00      1.00      1.00         1\n",
      "weighted avg       1.00      1.00      1.00         1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Sample dataset\n",
    "emails = np.array([\n",
    "    [1, 0, 1],  # \"Offer\", \"Win\"\n",
    "    [0, 1, 0],  # \"Free\"\n",
    "    [1, 1, 1],  # \"Offer\", \"Free\", \"Win\"\n",
    "    [0, 0, 0],  # None\n",
    "    [1, 1, 0]   # \"Offer\", \"Free\"\n",
    "])\n",
    "\n",
    "labels = np.array([1, 0, 1, 0, 0])  # 1 = Spam, 0 = Not Spam\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(emails, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the Naive Bayes classifier\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print('Classification Report:')\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a6a30c-7da6-4143-a684-512927efbf5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
