{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style = \"color : MediumTurquoise\"> Support Vector Machine </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b style = \"color : orangered; font-size : 15px\">Support Vector Machine (SVM) is a supervised learning algorithm used primarily for classification tasks, though it can also be used for regression.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style = \"color : dodgerblue\"> What is an SVM? </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b style = \"color : Tomato; font-size : 125%\">An SVM finds a hyperplane that best separates the classes in the feature space. It aims to maximize the margin between the classes.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style = \"color : dodgerblue\"> Key Concepts </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b style = \"color : Coral; font-size : 18px\"> 1. Hyperplane: </b>\n",
    "\n",
    "* A hyperplane is a decision boundary that separates different classes in the feature space.\n",
    "\n",
    "* In a 2D space, it's a line; in a 3D space, it's a plane; and in higher dimensions, it becomes a hyperplane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b style = \"color : Coral; font-size : 18px\"> 2. Support Vectors: </b>\n",
    "\n",
    "* Support vectors are the data points that are closest to the hyperplane. \n",
    "\n",
    "* These points are crucial as they influence the position and orientation of the hyperplane.\n",
    "\n",
    "* The SVM algorithm uses these support vectors to maximize the margin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b style = \"color : Coral; font-size : 18px\"> 3. Margin: </b>\n",
    "\n",
    "* The margin is the distance between the hyperplane and the closest support vector from either class.\n",
    "\n",
    "* SVM aims to maximize this margin to achieve better generalization on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b style = \"color : Coral; font-size : 18px\"> 4. Kernel Trick: </b>\n",
    "\n",
    "* When data is not linearly separable in its original space, SVM can use kernel functions to transform the data into a higher-dimensional space where a hyperplane can be used to separate the classes.\n",
    "\n",
    "<b style = \"color : teal\"> Common kernels include: </b>\n",
    "\n",
    "<b style = \"color : aquamarine\">1. Linear Kernel:</b> Good for linearly separable data.\n",
    "\n",
    "<b style = \"color : aquamarine\">2. Polynomial Kernel:</b> Useful when the data has polynomial relationships.\n",
    "\n",
    "<b style = \"color : aquamarine\">3. Radial Basis Function (RBF) Kernel:</b> Effective for non-linear data, mapping samples into higher-dimensional space.\n",
    "\n",
    "<b style = \"color : aquamarine\">4. Sigmoid Kernel:</b> Used in neural networks, similar to a two-layer perceptron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b style = \"color : Coral; font-size : 18px\"> 5. Hard Margin: </b>\n",
    "\n",
    "* A hard margin refers to the maximum-margin hyperplane that perfectly separates the data points of different classes without any misclassifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b style = \"color : Coral; font-size : 18px\"> 6. Soft Margin: </b>\n",
    "\n",
    "* When data contains outliers or is not perfectly separable, SVM uses the soft margin technique. \n",
    "\n",
    "* This method introduces a slack variable for each data point to allow some misclassifications while balancing between maximizing the margin and minimizing violations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b style = \"color : Coral; font-size : 18px\"> 7. C: </b>\n",
    "\n",
    "* The C parameter in SVM is a regularization term that balances margin maximization and the penalty for misclassifications.\n",
    "\n",
    "* A higher C value imposes a stricter penalty for margin violations, leading to a smaller margin but fewer misclassifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b style = \"color : Coral; font-size : 18px\"> 8. Hinge Loss: </b>\n",
    "\n",
    "* The hinge loss is a common loss function in SVMs.\n",
    "\n",
    "* It penalizes misclassified points or margin violations and is often combined with a regularization term in the objective function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b style = \"color : Coral; font-size : 18px\"> 9. Dual Problem: </b>\n",
    "\n",
    "* The dual problem in SVM involves solving for the Lagrange multipliers associated with the support vectors.\n",
    "\n",
    "* This formulation allows for the use of the kernel trick and facilitates more efficient computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style = \"color : dodgerblue\"> Types of Support Vector Machine </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style = \"color : RoyalBlue\"> 1. Linear SVM </h3>\n",
    "\n",
    "<b style = \"color : coral\"> i. Description: </b> Used when the data is linearly separable, meaning we can draw a straight line (or hyperplane in higher dimensions) to separate the classes.\n",
    "\n",
    "<b style = \"color : coral\"> ii. Example: </b> When we have two classes in a two-dimensional space that can be separated by a single line.\n",
    "\n",
    "<b style = \"color : coral\"> iii. Kernel: </b> Linear kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style = \"color : RoyalBlue\"> 2. Non-Linear SVM </h3>\n",
    "\n",
    "<b style = \"color : coral\"> i. Description: </b> Used when the data is not linearly separable. These SVMs use kernel functions to map the input features into higher-dimensional spaces where a hyperplane can separate the classes.\n",
    "\n",
    "<b style = \"color : coral\"> ii. Example: </b> When the classes form concentric circles in a two-dimensional space.\n",
    "\n",
    "<b style = \"color : coral\"> iii. Kernel: </b> Polynomial, Radial Basis Function (RBF), Sigmoid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style = \"color : RoyalBlue\"> 3. Support Vector Regression (SVR) </h3>\n",
    "\n",
    "<b style = \"color : coral\"> i. Description: </b> An extension of SVM for regression tasks. SVR aims to find a function that deviates from the actual observed values by a value no greater than a specified margin.\n",
    "\n",
    "<b style = \"color : coral\"> ii. Example: </b> Predicting continuous values like housing prices or stock prices.\n",
    "\n",
    "<b style = \"color : coral\"> iii. Kernel: </b> Linear, Polynomial, RBF, Sigmoid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style = \"color : RoyalBlue\"> 4. Nu-Support Vector Classification (Nu-SVC) </h3>\n",
    "\n",
    "<b style = \"color : coral\"> i. Description: </b> Similar to SVC but introduces a parameter nu which controls the number of support vectors and the margin.\n",
    "\n",
    "<b style = \"color : coral\"> ii. Example: </b> Useful when we need more control over the trade-off between the margin size and the number of support vectors.\n",
    "\n",
    "<b style = \"color : coral\"> iii. Kernel: </b> Same as SVC (Linear, Polynomial, RBF, Sigmoid)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style = \"color : RoyalBlue\"> 5. One-Class SVM </h3>\n",
    "\n",
    "<b style = \"color : coral\"> i. Description: </b> Used for anomaly detection. It identifies data points that are different from the majority of the data.\n",
    "\n",
    "<b style = \"color : coral\"> ii. Example: </b> Detecting fraudulent transactions or abnormal network traffic.\n",
    "\n",
    "<b style = \"color : coral\"> iii. Kernel: </b> Typically RBF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style = \"color : dodgerblue\"> Advantages of Support Vector Machine (SVM) </h2>\n",
    "\n",
    "<b style = \"color : coral\"> 1. High-Dimensional Performance: </b> SVM excels in high-dimensional spaces, making it suitable for image classification and gene expression analysis.\n",
    "\n",
    "<b style = \"color : coral\"> 2. Nonlinear Capability: </b> Utilizing kernel functions like RBF and polynomial, SVM effectively handles nonlinear relationships.\n",
    "\n",
    "<b style = \"color : coral\"> 3. Outlier Resilience: </b> The soft margin feature allows SVM to ignore outliers, enhancing robustness in spam detection and anomaly detection.\n",
    "\n",
    "<b style = \"color : coral\"> 4. Binary and Multiclass Support: </b> SVM is effective for both binary classification and multiclass classification, suitable for applications in text classification.\n",
    "\n",
    "<b style = \"color : coral\"> 5. Memory Efficiency: </b> SVM focuses on support vectors, making it memory efficient compared to other algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style = \"color : dodgerblue\"> Disadvantages of Support Vector Machine (SVM) </h2>\n",
    "\n",
    "<b style = \"color : coral\"> 1. Slow Training: </b> SVM can be slow for large datasets, affecting performance in SVM in data mining tasks.\n",
    "\n",
    "<b style = \"color : coral\"> 2. Parameter Tuning Difficulty: </b> Selecting the right kernel and adjusting parameters like C requires careful tuning, impacting SVM algorithms.\n",
    "\n",
    "<b style = \"color : coral\"> 3. Noise Sensitivity: </b> SVM struggles with noisy datasets and overlapping classes, limiting effectiveness in real-world scenarios.\n",
    "\n",
    "<b style = \"color : coral\"> 4. Limited Interpretability: </b> The complexity of the hyperplane in higher dimensions makes SVM less interpretable than other models.\n",
    "\n",
    "<b style = \"color : coral\"> 5. Feature Scaling Sensitivity: </b> Proper feature scaling is essential; otherwise, SVM models may perform poorly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style = \"color : dodgerblue\"> Conclusion </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In conclusion, Support Vector Machines (SVM) are powerful algorithms in machine learning, ideal for both classification and regression tasks. They excel at finding the optimal hyperplane for separating data, making them suitable for applications like image classification and anomaly detection.\n",
    "\n",
    "* SVM's adaptability through kernel functions allows it to handle both linear and nonlinear data effectively. However, challenges like parameter tuning and potential slow training times on large datasets must be considered.\n",
    "\n",
    "* Understanding SVM is crucial for data scientists, as it enhances predictive accuracy and decision-making across various domains, including data mining and artificial intelligence."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
